# ESG PDF 段落向量資料庫與 Agent A 檢索流程

本文件說明資料建立流程與 Agent A 使用方式，重點包含：
- **Chunk 會帶 Company / Year / Page 等 metadata**
- **公司名稱可由前端選單限制輸入**（降低別名造成的過濾失敗）
- Agent A 會在檢索後進行 **公司過濾與公司排名選擇（company_ranked）**

---

## 1. PDF 文本擷取與清理（PyMuPDF + Normalization）

### 1.1 文本擷取
使用 **PyMuPDF (fitz)** 逐頁解析 PDF，取出可檢索文字內容。

### 1.2 文字正規化（Normalization）
針對 PDF 常見噪音做清理（多餘空白、換行等），避免干擾後續斷句與語意向量。

> 目的：讓輸入 embedding 的 chunk 文字更乾淨、語意更穩定。

---

## 2. 斷句與分塊策略（Sentence Split + Sliding Window）

### 2.1 斷句（Sentence Split）
以標點符號（如 `。；！？`）切成句子，並做 whitespace normalize。

### 2.2 分塊（Chunking）
採用 **滑動視窗（window=3, stride=1）** 組合句子成 chunk，並加入長度限制：

- `chunk = 3 句組合`
- 長度條件：`50 <= len(chunk) <= 800`

> 這種 chunk 方式能保留局部上下文，對「承諾句」或「政策描述」類段落通常更友善。

---

## 3. Metadata 設計（Company / Year / Page）

每個 chunk 都會寫入必要的溯源資訊至少包含：

- `company`：公司（建議是 canonical/選單一致名稱）
- `year`：年度（優先由檔名抽取；必要時可由 PDF 首頁猜測）
- `page`：頁碼
- `pdf`：原始 PDF 路徑
- `source_stem`：原始檔名（方便追溯）

範例（meta.json 中每筆 record）：

```json
{
  "company": "中油",
  "year": 2024,
  "source_stem": "台灣中油2024_永續報告書",
  "pdf": "data/台灣中油2024_永續報告書.pdf",
  "page": 12,
  "chunk": "……"
}
```

## 4. 多語言向量化（Sentence-Transformers）

### 4.1 使用模型

採用 **Sentence-Transformers** 多語言模型：

- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

**特性：**
- 支援中英文混合語料  
- 對 paraphrase（不同問法找同概念）效果穩定  
- 可在 CPU 上運行（較輕量）  

### 4.2 Cosine Similarity 設定

編碼時使用 `normalize_embeddings=True`，使得：
- `IndexFlatIP`（內積）≈ `cosine similarity`（餘弦相似度）

### 5.1 `faiss.index`
- **FAISS Index**：`IndexFlatIP(dim)`
- **用途**：儲存高維向量，用於 **Top-K 相似度檢索**
- **相似度計算**：
  - 搭配 `normalize_embeddings=True` 時，`IndexFlatIP` 的內積結果可視為 **Cosine Similarity**

### 5.2 `meta.json`

- **用途**：儲存每個向量對應的文字 `chunk` 與 metadata（如 `company/year/page/pdf/source_stem`）
- **查詢方式**：
  1. 先由 FAISS 回傳最相似的向量 id
  2. 再用該 id 回查 `meta.json` 取得原文段落與溯源資訊

---

## 6. Agent A 檢索流程

### Step 1：組合查詢字串（Query Expansion）
```python
full_query = f"{company} {query}"
```
### Step 2：向量檢索（Vector Search）
- **查詢向量化**
  - 將 `full_query` 轉為向量（Embedding）
  - 建議使用與建庫一致的模型與設定（`normalize_embeddings=True`）

- **FAISS Top-K 檢索**
  - 在 `faiss.index` 中以 `IndexFlatIP` 搜尋最相似的 Top-K chunks
  - 取得結果包含：
    - `topk_ids`：向量 id（對應 `meta.json` 的 row index / meta_id）
    - `topk_scores`：相似度分數（內積；已正規化時≈ cosine similarity）

- **回查原文與溯源資料**
  - 依 `topk_ids` 回查 `meta.json` 取得：
    - `chunk`（原文段落）
    - `company / year / page / pdf / source_stem` 等 metadata

---

### Step 3：公司過濾（Company Filtering）

- **候選 passages 過濾**
  - 取得 Top-K 候選 passages 後，依使用者指定的公司（`preferred_company`）進行過濾
  - 建議做法：
    - 以 **寬鬆比對**（normalize + 包含關係）降低因格式差異導致的漏抓
    - 或以前端選單 `value` 作為一致的公司鍵（見 §7）

- **公司聚合與排序（company_ranked）**
  - 對過濾後 passages 依 `company` 做聚合
  - 彙總每家公司在候選集中累積的相似度分數（例如：加總 top-k scores）
  - 產生 `company_ranked`（公司 → 累積分數）並排序，挑出最可能的目標公司

- **最終選定公司與 passages 抽取**
  - 取 `company_ranked` 第一名作為「最後選中的公司」
  - 從該公司 passages 中抽取固定數量（例如 `N=30`）作為 LLM context
  - 抽取策略可包含：
    - 依相似度分數由高到低取前 N
    - 或去重（避免高度相似 chunk 重複出現）
    - 或限制同頁/同段落重複（提升 context 多樣性）

#### 實際 log 行為（示例）

- `retrieve_all` 取回 `500` 筆
- 使用寬鬆比對針對「台灣中油」過濾後剩 `403` 筆
- `company_ranked`（filtered）= `[('中油', 217.1596...)]`
- 最後選中的公司 = `中油`
- 對 `中油` 取出 `30` 個 passages

---

### Step 4：組合 Context 並呼叫 LLM

- **Context 組裝（Context Assembly）**
  - 將抽取的 passages 整理成結構化 context（建議包含 metadata）
  - 每段_toggle_建議附上：
    - `meta_id`（或 row_id）
    - `company / year / page`
    - `chunk` 原文

- **LLM 推論（Ollama/LLM Inference）**
  - 呼叫 Ollama/LLM 生成答案
  - Prompt 建議要求：
    - 僅能依據 context 作答
    - 需要引用來源（`page / year / meta_id` 等）以利溯源
    - 不足資訊時需明確回答「找不到/無證據」

- **輸出（Answer + Citations）**
  - 回答內容建議結構：
    - 結論/承諾敘述
    - 證據摘錄（可短引）
    - 引用標記（`meta_id` + `page/year`）

---
